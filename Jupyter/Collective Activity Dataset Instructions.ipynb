{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Reconstruction for Collective Activity Dataset (CAD-2)\n",
    "**Author:** Christian Byron  **Date:** 19-Apr-21\n",
    "\n",
    "This notebook provides an understanding of the Collective Activity dataset by reconstructing it into a video gui. \n",
    "\n",
    "The Collective Activity Dataset (available [here](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)) was created by <sub>[1]</sub> \n",
    "\n",
    "- [ ] get the markup corrected to put the annotation description into a table \n",
    "\n",
    "\n",
    "- The original dataset contains 5 different collective activities : crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.\n",
    "- The augmented dataset (used here) added two more categories (dancing and jogging)\n",
    "\n",
    "#### Annotation\n",
    "\n",
    "Every 10th frame in all video sequences was annotated with image location of person, activity id, and pose direction.\n",
    "\n",
    "Frame number, X, Y, WIDTH, HEIGHT, CLASS ID, POSE ID.\n",
    "ex. 001       366     168     106     212     5       3\n",
    "    001     512     190     98      195     5       3\n",
    "    001     440     187     84      167     5       3\n",
    "    001     339     191     83      165     5       3\n",
    "\n",
    "CLASS ID \n",
    "1. NA, 2. Crossing, 3. Waiting, 4.Queuing, 5. Walking (not use), 6. Talking, 7. Dancing, 8. Jogging\n",
    "\n",
    "POSE ID\n",
    "1. Right 2. Front-right 3. Front 4. Front-left 5. Left 6. Back-left 7. Back 8. Back-right\n",
    "\n",
    "\n",
    "### Step 1 - Find all videos and individual frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "base_video_collection_path = \\\n",
    "    \"C:\\\\Users\\\\s441606\\\\OneDrive - University of Canberra\\\\2021 Semester 1\\\\z- Learning Python\\\\Jupyter\\\\videos\"\n",
    "\n",
    "video_folder_list = os.listdir(base_video_collection_path)\n",
    "videos = {}\n",
    "\n",
    "for video_folder in video_folder_list:\n",
    "\n",
    "    video_starting_frame = sys.maxsize\n",
    "    video_ending_frame = 0\n",
    "    video_frames = {}\n",
    "    video_annotations_all = {}\n",
    "    video_annotations_options = list()\n",
    "    \n",
    "    video_annotations_file = open(base_video_collection_path + \"\\\\\" + video_folder + '\\\\annotations.txt', 'r')\n",
    "    \n",
    "    for video_annotations_line in video_annotations_file:\n",
    "        video_annotation_line_search = re.search('\\d+', video_annotations_line)\n",
    "        if video_annotation_line_search: video_annotations_all.update({video_annotation_line_search.group(0): \n",
    "                                                                       video_annotations_line})\n",
    "    \n",
    "    \n",
    "    for subdir, dirs, files in os.walk(base_video_collection_path + \"\\\\\" + video_folder):\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                video_frame_number = os.path.splitext(file)[0]\n",
    "                video_starting_frame = min(int(video_frame_number), video_starting_frame)\n",
    "                video_ending_frame = max(int(video_frame_number), video_ending_frame)\n",
    "                video_frames.update({int(video_frame_number):os.path.join(subdir, file)})\n",
    "                \n",
    "                if video_frame_number in video_annotations_all :\n",
    "                    video_annotation_line_split = re.split('\\s', video_annotations_all[video_frame_number])\n",
    "                    video_annotation_line_split[0] = video_frame_number\n",
    "                    video_annotations_options.append((video_frame_number + ' ' + video_annotation_line_split[1], \n",
    "                                                      video_annotation_line_split))\n",
    "               \n",
    "    video_obj = {\"starting_frame\":video_starting_frame, \"ending_frame\":video_ending_frame,\"frames\":video_frames,\n",
    "                 \"annotations_options\":video_annotations_options} \n",
    "    \n",
    "    videos.update({video_folder:video_obj})\n",
    "\n",
    "#display(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Build the GUI Function to show video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11e61a5b5a545c7a24c2157a2f62849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Video #:', options=('0', '1'), value='0'), Play(value=3576…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "imageArea = widgets.Image()\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options= video_folder_list,\n",
    "    value ='1',\n",
    "    description='Video #:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "play = widgets.Play(\n",
    "        step=1,\n",
    "        interval=50,\n",
    "        description=\"Press play\",\n",
    "        disabled=False\n",
    ")\n",
    "\n",
    "slider = widgets.IntSlider()\n",
    "\n",
    "widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "\n",
    "def on_dropdown_value_change(change): \n",
    "    video_selected = videos[change['new']]\n",
    "    video_start = video_selected[\"starting_frame\"]\n",
    "    video_end = video_selected[\"ending_frame\"]\n",
    "    \n",
    "    slider.min = play.min = 0\n",
    "    slider.max = play.max = video_end\n",
    "    slider.min = play.min = video_start\n",
    "    slider.value = video_start\n",
    "    \n",
    "    annotations.options = video_selected['annotations_options']\n",
    "  \n",
    "  \n",
    "dropdown.observe(on_dropdown_value_change, names='value')\n",
    "\n",
    "def on_value_change(change):\n",
    "    video_selected = videos[dropdown.value]\n",
    "    \n",
    "    file = open(f\"{video_selected['frames'][change['new']]}\",\"rb\")\n",
    "    imageArea.value = file.read()\n",
    " \n",
    "slider.observe(on_value_change, names='value')\n",
    "\n",
    "annotations = widgets.Dropdown(\n",
    "    options= videos[dropdown.value]['annotations_options'],\n",
    "    description='Annotated Frame #:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "controls = widgets.VBox([widgets.HBox([dropdown, play, slider,annotations]),imageArea])\n",
    "\n",
    "# trigger the initial video to correctly set the slider values and load the first image\n",
    "dropdown.value = '0'\n",
    "controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Update the display when an Annotation is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def on_annotations_dropdown_value_change(change): \n",
    "    annotations_list = change['new']\n",
    "    frame_number = int(annotations_list[0])\n",
    "    play._playing = False\n",
    "    slider.value = frame_number\n",
    "    \n",
    "    # convert the current frame image to a numpy array for opencv\n",
    "    nparr = np.frombuffer(imageArea.value, np.uint8)\n",
    "    im = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    #loop through each annotation set in the list 9 set of five values each\n",
    "    for i in range(2,len( annotations_list) -5 ,5):\n",
    "        start_point = (int(annotations_list[i]), int(annotations_list[i+1]))\n",
    "\n",
    "        end_point = (int(annotations_list[i]) + int(annotations_list[i+2]), \n",
    "                     int(annotations_list[i+1]) + int(annotations_list[i+3]))\n",
    "        \n",
    "        im = cv2.rectangle(im, start_point, end_point, (255, 0, 0), 2)\n",
    "        im = cv2.putText(im, annotations_list[i+4], start_point, cv2.FONT_HERSHEY_SIMPLEX , \n",
    "                         1, (255, 0, 0), 1)\n",
    "        \n",
    "   \n",
    "    is_success, im_buf_arr = cv2.imencode(\".jpg\", im)\n",
    "    byte_im = im_buf_arr.tobytes()\n",
    "\n",
    "    imageArea.value = byte_im\n",
    "    \n",
    "    \n",
    "\n",
    "annotations.observe(on_annotations_dropdown_value_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lessons Learnt\n",
    "- A lot of learning about the iPython widgets (inc lack of documentation on class interfaces). Especially on how to use observe method to select and run the video(cascading value changes)\n",
    "- How to show a image using ipyWidgets - may be useful later - or may be replaced with other image libraries (eg Matlablib)\n",
    "- A bit of a challenge to create the complex store of videos in a dictionary\n",
    "- Folders are named after the annotated frame - not the beginning or end. So need to process differently to get the correct details\n",
    "- Refresh syntax for regular expressions - used https://regex101.com/ as means to test and the best cheatsheet https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions/Cheatsheet\n",
    "- Started with openCV and use in jupyter - discovered natively cv2.imshow opens new windows :-P ; where as to open in Image widget needs conversion from bytes to numpy array (useful sites https://jdhao.github.io/2019/07/06/python_opencv_pil_image_to_bytes/ and https://intellipaat.com/community/15647/python-opencv-load-image-from-byte-string\n",
    "\n",
    "\n",
    "#### References\n",
    "[1]  Mostafa S. Ibrahim et al. “A Hierarchical Deep Temporal Model for Group Activity Recognition”. In:2016 IeeeConference on Computer Vision and Pattern Recognition.  IEEE  Conference  on  Computer  Vision  and  PatternRecognition.  2016,  pp.  1971–1980.isbn:  978-1-4673-8851-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
