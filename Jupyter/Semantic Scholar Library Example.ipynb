{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Semantic Scholar library\n",
    "**Author:** Christian Byron  **Date:** 30-Mar-21\n",
    "\n",
    "This demo extracts data from the [Semantic Scholar API](https://api.semanticscholar.org/) using the python library `semanticscholar`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 - Create a list of Digital Object Identifiers (DOI)\n",
    "- [x] Later read this in from a file (possibly scraping from a bibtex format)\n",
    "- [ ] Need to cater for different formats of the bibtex fields (DOI vs doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DOI_List = []\n",
    "\n",
    "BibTex_File = open('temp.bib', 'r') \n",
    "\n",
    "for BibTex_Line in BibTex_File:\n",
    "    if BibTex_Line[:6] == '   DOI' :\n",
    "        DOI_Search = re.search('{(.+?)}', BibTex_Line)\n",
    "        if DOI_Search: DOI_List.append(DOI_Search.group(1))\n",
    "        \n",
    "BibTex_File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Loop through the list and store the citation data from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import semanticscholar as sch\n",
    "\n",
    "papers = []\n",
    "\n",
    "for DOI in DOI_List:\n",
    "    paper = sch.paper(DOI, timeout=2)\n",
    "    papers.append(paper)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Loop through the resulting citation data and produce a table\n",
    "- [ ] Need to handle case where schemantics scholar returns null as a aritcle cannot be found (eg recent publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>DOI</th><th>Title</th><th>Citation Count</th><tr><td>10.1109/AINA.2016.94</td><td>Energy Considerations for Continuous Group Activity Recognition Using Mobile Devices: The Case of GroupSense</td><td>9</td></tr><tr><td>10.1186/s13174-019-0103-1</td><td>GARSAaaS: group activity recognition and situation analysis as a service</td><td>3</td></tr><tr><td>10.1145/3295747</td><td>GroupSense</td><td>0</td></tr><tr><td>10.1007/978-3-030-30645-8_26</td><td>Mask Guided Fusion for Group Activity Recognition in Images</td><td>1</td></tr><tr><td>10.1145/3316615.3316722</td><td>Cooperative Hierarchical Framework for Group Activity Recognition: From Group Detection to Multi-activity Recognition</td><td>1</td></tr><tr><td>10.1109/CVPR.2019.00808</td><td>Convolutional Relational Machine for Group Activity Recognition</td><td>19</td></tr><tr><td>10.1007/s00521-016-2346-0</td><td>Constrained self-organizing feature map to preserve feature extraction topology</td><td>2</td></tr><tr><td>10.1109/MDM.2014.62</td><td>A Framework for Continuous Group Activity Recognition Using Mobile Devices: Concept and Experimentation</td><td>6</td></tr><tr><td>10.1109/COMPSAC.2016.202</td><td>Temporal Dependency Rule Learning Based Group Activity Recognition in Smart Spaces</td><td>8</td></tr><tr><td>10.1016/J.AUTCON.2019.102886</td><td>Two-step long short-term memory method for identifying construction activities through positional and attentional cues</td><td>13</td></tr><tr><td>10.1145/3311957.3359471</td><td>Group Activity Recognition to Support Collaboration in Creative Digital Space</td><td>0</td></tr><tr><td>10.1142/S0218001415550071</td><td>Group Activity Recognition with Group Interaction Zone Based on Relative Distance Between Human Objects</td><td>30</td></tr><tr><td>10.1109/CVPR.2016.516</td><td>Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</td><td>155</td></tr><tr><td>10.1117/12.919946</td><td>Team activity analysis and recognition based on Kinect depth map and optical imagery techniques</td><td>7</td></tr><tr><td>10.1117/12.2018626</td><td>A robust technique for semantic annotation of group activities based on recognition of extracted features in video streams</td><td>5</td></tr><tr><td>10.1117/12.2050909</td><td>Knowledge discovery in group activities through sequential observation analysis</td><td>5</td></tr><tr><td>10.1016/j.procs.2019.11.212</td><td>Group Activity Recognition Method based on Camera in The Building</td><td>0</td></tr><tr><td>10.3390/fi12080133</td><td>Multimodal Deep Learning for Group Activity Recognition in Smart Office Environments</td><td>1</td></tr><tr><td>10.1007/978-3-030-20887-5_21</td><td>Multi-Level Sequence GAN for Group Activity Recognition</td><td>11</td></tr><tr><td>10.1109/ICCV.2003.1238423</td><td>Recognition of group activities using dynamic probabilistic networks</td><td>299</td></tr><tr><td>10.1007/s11036-012-0415-x</td><td>Towards Collaborative Group Activity Recognition Using Mobile Devices</td><td>38</td></tr><tr><td>10.1109/TPAMI.2016.2613865</td><td>Multi-Instance Classification by Max-Margin Training of Cardinality-Based Markov Networks</td><td>10</td></tr><tr><td>10.1109/ICCV.2015.462</td><td>Learning Ensembles of Potential Functions for Structured Prediction with Latent Variables</td><td>9</td></tr><tr><td>10.1007/978-3-030-01219-9_44</td><td>Hierarchical Relational Networks for Group Activity Recognition and Retrieval</td><td>37</td></tr><tr><td>10.1109/CVPR.2016.217</td><td>A Hierarchical Deep Temporal Model for Group Activity Recognition</td><td>216</td></tr><tr><td>10.1007/978-3-319-24078-7_16</td><td>Recognition of Human Group Activity for Video Analytics</td><td>1</td></tr><tr><td>10.1016/j.patcog.2017.10.037</td><td>Discriminative context learning with gated recurrent unit for group activity recognition</td><td>26</td></tr><tr><td>10.1109/ICPR.2014.605</td><td>Group Activity Recognition with Group Interaction Zone</td><td>14</td></tr><tr><td>10.1109/TPAMI.2011.228</td><td>Discriminative Latent Models for Recognizing Contextual Group Activities</td><td>236</td></tr><tr><td>10.1109/WACV.2017.31</td><td>Recognition of Group Activities in Videos Based on Single-and Two-Person Descriptors</td><td>7</td></tr><tr><td>10.1007/s11263-012-0573-0</td><td>Recognizing Interactive Group Activities Using Temporal Interaction Matrices and Their Riemannian Statistics</td><td>19</td></tr><tr><td>10.1109/ICCV.2017.313</td><td>SBGAR: Semantics Based Group Activity Recognition</td><td>37</td></tr><tr><td>10.1109/TCSVT.2013.2280849</td><td>A New Network-Based Algorithm for Human Activity Recognition in Videos</td><td>17</td></tr><tr><td>10.1109/TCSVT.2013.2269780</td><td>A Heat-Map-Based Algorithm for Recognizing Group Activities in Videos</td><td>39</td></tr><tr><td>10.1109/ACCESS.2019.2929684</td><td>Deep Fully Connected Model for Collective Activity Recognition</td><td>3</td></tr><tr><td>10.1109/ICCVW.2013.71</td><td>Temporal Poselets for Collective Activity Detection and Recognition</td><td>18</td></tr><tr><td>10.1117/12.2178547</td><td>Physical environment virtualization for human activities recognition</td><td>6</td></tr><tr><td>10.1007/978-3-030-01249-6_7</td><td>stagNet: An Attentive Semantic RNN for Group Activity Recognition</td><td>37</td></tr><tr><td>10.1109/TCSVT.2019.2894161</td><td>stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition</td><td>15</td></tr><tr><td>10.1007/s12652-020-01851-0</td><td>Working together: a DBN approach for individual and group activity recognition</td><td>2</td></tr><tr><td>10.1007/s11263-010-0355-5</td><td>Stochastic Representation and Recognition of High-Level Group Activities</td><td>117</td></tr><tr><td>10.1117/12.2266224</td><td>Joint object and action recognition via fusion of partially observable surveillance imagery data</td><td>1</td></tr><tr><td>10.1117/12.2226026</td><td>Skin subspace color modeling for daytime and nighttime group activity recognition in confined operational spaces</td><td>4</td></tr><tr><td>10.1117/12.2226029</td><td>In-vehicle group activity modeling and simulation in sensor-based virtual environment</td><td>5</td></tr><tr><td>10.1109/CVPR.2017.453</td><td>CERN: Confidence-Energy Recurrent Network for Group Activity Recognition</td><td>63</td></tr><tr><td>10.1007/978-3-319-64689-3_7</td><td>Recognizing Interactions Between People from Video Sequences</td><td>0</td></tr><tr><td>10.1109/TIP.2019.2914577</td><td>Learning Semantics-Preserving Attention and Contextual Interaction for Group Activity Recognition</td><td>7</td></tr><tr><td>10.1145/3240508.3240576</td><td>Mining Semantics-Preserving Attention for Group Activity Recognition</td><td>12</td></tr><tr><td>10.1109/TCSVT.2020.2973301</td><td>Graph Interaction Networks for Relation Transfer in Human Activity Videos</td><td>0</td></tr><tr><td>10.1109/CVPRW.2017.24</td><td>Classification of Puck Possession Events in Ice Hockey</td><td>25</td></tr><tr><td>10.1016/j.patrec.2013.09.015</td><td>Activity analysis in crowded environments using social cues for group discovery and human interaction modeling</td><td>43</td></tr><tr><td>10.1109/CVPR.2017.783</td><td>Recurrent Modeling of Interaction Context for Collective Activity Recognition</td><td>50</td></tr><tr><td>10.1002/TEE.22901</td><td>Group activity recognition with an interaction force based on low‚Äêlevel features</td><td>2</td></tr><tr><td>10.1109/CVPR.2019.01020</td><td>Learning Actor Relation Graphs for Group Activity Recognition</td><td>43</td></tr><tr><td>10.1016/j.neucom.2020.07.003</td><td>Fusing Motion Patterns and Key Visual Information for Semantic Event Recognition in Basketball Videos</td><td>0</td></tr><tr><td>10.1016/J.JVCIR.2019.01.006</td><td>Jointly social grouping and identification in visual dynamics with causality-induced hierarchical Bayesian model</td><td>10</td></tr><tr><td>10.1109/ACCESS.2020.2979742</td><td>Group Activity Recognition by Using Effective Multiple Modality Relation Representation With Temporal-Spatial Attention</td><td>0</td></tr><tr><td>10.1145/3240508.3240572</td><td>Participation-Contributed Temporal Dynamic Model for Group Activity Recognition</td><td>17</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML('<table><tr><th>DOI</th><th>Title</th><th>Citation Count</th><tr>{}</tr></table>'.format(\n",
    "                '</tr><tr>'.join(\n",
    "                    '<td>{}{}{}{}{}</td>'.format(paper['doi'], '</td><td>', paper['title'] , \n",
    "                                                 '</td><td>', len(paper['citations']) )\n",
    "                for paper in papers)\n",
    "            )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lessons Learnt\n",
    "\n",
    "- Ways to open an file and read it line by line, in contrast to reading in every line into a list or string\n",
    "- Beginning in using regular expressions is relatively straigthtforward\n",
    "- Displaying HTML and table is potentially useful\n",
    "- Debugging in Jupyter is painful - need to look at other ways to write python initally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(papers)-1):\n",
    "#        print(f\"{i} - {papers[i]['title']}\")\n",
    "\n",
    "#print(papers[54])\n",
    "print(DOI_List)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
