{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Semantic Scholar library\n",
    "**Author:** Christian Byron  **Date:** 30-Mar-21\n",
    "\n",
    "This demo extracts data from the [Semantic Scholar API](https://api.semanticscholar.org/) using the python library `semanticscholar`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 - Create a list of Digital Object Identifiers (DOI)\n",
    "- [x] Later read this in from a file (possibly scraping from a bibtex format)\n",
    "- [ ] Need to cater for different formats of the bibtex fields (DOI vs doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abkenar, Amin B.', 'Loke, Seng W.', 'Rahayu, Wenny', 'Zaslaysky, Arkady']\n",
      "['Abkenar, Amin B.', 'Loke, Seng W.', 'Zaslavsky, Arkady', 'Rahayu, Wenny']\n",
      "['Abkenar, Amin B.', 'Loke, Seng W.', 'Zaslavsky, Arkady', 'Rahayu, Wenny']\n",
      "['Akar, Arif', 'Ikizler-Cinbis, Nazli']\n",
      "['Al-Habib, Mohammed', 'Huang, Dongjun', 'Al-Qatf, Majjed', 'Al-Sabahi, Kamal', 'Acm']\n",
      "['Antic, Borislav', 'Ommer, Bjoern']\n",
      "['Azar, Sina Mokhtarzadeh', 'Atigh, Mina Ghadimi', 'Nickabadi, Ahmad', 'Alahi, Alexandre', 'Soc, Ieee Comp']\n",
      "['Azorin-Lopez, Jorge', 'Saval-Calvo, Marcelo', 'Fuster-Guillo, Andres', 'Garcia-Rodriguez, Jose', 'Mora-Mora, Higinio']\n",
      "['BakhshandehAbkenar, Amin', 'Loke, Seng W.', 'Rahayu, Wenny', 'Ieee']\n",
      "['Bourbia, Amine Lotfi', 'Son, Heesuk', 'Shin, Byoungheon', 'Kim, Taehun', 'Lee, Dongman', 'Hyun, Soon J.']\n",
      "['Cai, Jiannan', 'Zhang, Yuxi', 'Cai, Hubo']\n",
      "['Casserfelt, Karl', 'Mihailescu, Radu-Casian', 'Ieee']\n",
      "['Chang, Xiaobin', 'Li, Xiang', 'Mai, Yuting', 'Zheng, Wei-Shi']\n",
      "['Chen, Bin', 'Hatada, Koki', 'Okabayashi, Keiju', 'Kuromiya, Hiroyuki', 'Hidaka, Ichiro', 'Yamamoto, Yoshiharu', 'Togami, Kazumasa', 'Acm']\n",
      "['Cho, Nam-Gyu', 'Kim, Young-Ji', 'Park, Unsang', 'Park, Jeong-Seon', 'Lee, Seong-Whan']\n",
      "['Choi, Jung-In', 'Yong, Hwan-Seung', 'Ieee']\n",
      "['Das, Snigdha', 'Ieee']\n",
      "['Deng, Zhiwei', 'Vandat, Arash', 'Hu, Hexiang', 'Mori, Greg', 'Ieee']\n",
      "['Elangovan, Vinayak', 'Bandaru, Vinod K.', 'Shirkhodaie, Amir']\n",
      "['Elangovan, Vinayak', 'Shirkhodaie, Amir']\n",
      "['Elangovan, Vinayak', 'Shirkhodaie, Amir']\n",
      "['Fauzi, Chairani', 'Sulistyo, Selo', 'Widyawan']\n",
      "['Florea, George Albert', 'Mihailescu, Radu-Casian']\n",
      "['Gammulle, Harshala', 'Denman, Simon', 'Sridharan, Sridha', 'Fookes, Clinton']\n",
      "['Gong, S. G.', 'Xiang, T.', 'Society, Ieee Computer', 'Ieee Computer, Society']\n",
      "['Gordon, Dawud', 'Hanne, Jan-Hendrik', 'Berchtold, Martin', 'Miyaki, Takashi', 'Beigl, Michael']\n",
      "['Gordon, Dawud', 'Hanne, Jan-Hendrik', 'Berchtold, Martin', 'Shirehjini, Ali Asghar Nazari', 'Beigl, Michael']\n",
      "['Hajimirsadeghi, Hossein', 'Mori, Greg']\n",
      "['Hajimirsadeghi, Hossein', 'Mori, Greg', 'Ieee']\n",
      "['Ibrahim, Mostafa S.', 'Mori, Greg']\n",
      "['Ibrahim, Mostafa S.', 'Muralidharan, Srikanth', 'Deng, Zhiwei', 'Vandat, Arash', 'Mori, Greg', 'Ieee']\n",
      "['Ju, Jaeyong', 'Yang, Cheoljong', 'Scherer, Sebastian', 'Ko, Hanseok']\n",
      "['Kim, Pil-Soo', 'Lee, Dong-Gyu', 'Lee, Seong-Whan']\n",
      "['Kim, Young-Ji', 'Cho, Nam-Gyu', 'Lee, Seong-Whan', 'Ieee']\n",
      "['Kong, Longteng', 'Qin, Jie', 'Huang, Di', 'Wang, Yunhong', 'Van Gool, Luc', 'Ieee']\n",
      "['Lan, Tian', 'Wang, Yang', 'Yang, Weilong', 'Robinovitch, Stephen N.', 'Mori, Greg']\n",
      "['Lathuiliere, Stephane', 'Evangelidis, Georgios', 'Horaud, Radu', 'Ieee']\n",
      "['Lee, Dong-Gyu', 'Kim, Pil-Soo', 'Lee, Seong-Whan', 'Ieee']\n",
      "['Li, Ruonan', 'Chellappa, Rama', 'Zhou, Shaohua Kevin']\n",
      "['Li, Ruonan', 'Chellappa, Rama', 'Zhou, Shaohua Kevin', 'Ieee']\n",
      "['Li, Xin', 'Chuah, Mooi Choo', 'Ieee']\n",
      "['Lin, Weiyao', 'Chen, Yuanzhe', 'Wu, Jianxin', 'Wang, Hanli', 'Sheng, Bin', 'Li, Hongxiang']\n",
      "['Lin, Weiyao', 'Chu, Hang', 'Wu, Jianxin', 'Sheng, Bin', 'Chen, Zhenzhong']\n",
      "['Liu, Jichao', 'Wang, Chuanxu', 'Gong, Yuting', 'Xue, Hao']\n",
      "['Meng, Lingxun', 'Qing, Laiyun', 'Yang, Peng', 'Miao, Jun', 'Chen, Xilin', 'Metaxas, Dimitris N.', 'Ieee']\n",
      "['Nabi, Moin', 'Del Bue, Alessio', 'Murino, Vittorio', 'Ieee']\n",
      "['Poshtyar, Azin', 'Elangovan, Vinayak', 'Shirkhodaie, Amir', 'Chan, Alex', 'Hu, Shuowen']\n",
      "['Qi, Mengshi', 'Qin, Jie', 'Li, Annan', 'Wang, Yunhong', 'Luo, Jiebo', 'Van Gool, Luc']\n",
      "['Qi, Mengshi', 'Wang, Yunhong', 'Qin, Jie', 'Li, Annan', 'Luo, Jiebo', 'Van Gool, Luc']\n",
      "['Rossi, Silvia', 'Acampora, Giovanni', 'Staffa, Mariacarla']\n",
      "['Rossi, Silvia', 'Capasso, Roberto', 'Acampora, Giovanni', 'Staffa, Mariacarla', 'Ieee']\n",
      "['Ryoo, M. S.', 'Aggarwal, J. K.']\n",
      "['Shirkhodaie, Amir', 'Chan, Alex L.']\n",
      "['Shirkhodaie, Amir', 'Poshtyar, Azin', 'Chan, Alex', 'Hu, Shuowen']\n",
      "['Shirkhodaie, Amir', 'Telagamsetti, Durga', 'Poshtyar, Azin', 'Chan, Alex', 'Hu, Shuowen']\n",
      "['Shu, Tianmin', 'Todorovic, Sinisa', 'Zhu, Song-Chun', 'Ieee']\n",
      "['Stephens, Kyle', 'Bors, Adrian G.']\n",
      "['Stephens, Kyle', 'Bors, Adrian G.', 'Ieee']\n",
      "['Stephens, Kyle', 'Bors, Adrian G.', 'Ieee']\n",
      "['Tang, Yansong', 'Lu, Jiwen', 'Wang, Zian', 'Yang, Ming', 'Zhou, Jie']\n",
      "['Tang, Yansong', 'Wang, Zian', 'Li, Peiyang', 'Lu, Jiwen', 'Yang, Ming', 'Zhou, Jie', 'Acm']\n",
      "['Tang, Yansong', 'Wei, Yi', 'Yu, Xumin', 'Lu, Jiwen', 'Zhou, Jie']\n",
      "['Tora, Moumita Roy', 'Chen, Jianhui', 'Little, James J.', 'Ieee']\n",
      "['Tran, K. N.', 'Gala, A.', 'Kakadiaris, I. A.', 'Shah, S. K.']\n",
      "['Vahora, S. A.', 'Chauhan, N. C.']\n",
      "['Vahora, SA', 'Chauhan, NC']\n",
      "['Wang, Minsi', 'Ni, Bingbing', 'Yang, Xiaokang', 'Ieee']\n",
      "['Wateosot, Chonthisa', 'Suvonvorn, Nikom']\n",
      "['Wu, Jianchao', 'Wang, Limin', 'Wang, Li', 'Guo, Jie', 'Wu, Gangshan', 'Soc, Ieee Comp']\n",
      "['Wu, Lifang', 'Yang, Zhou', 'Wang, Qi', 'Jian, Meng', 'Zhao, Boxuan', 'Yan, Junchi', 'Chen, Chang Wen']\n",
      "['Xie, Zhao', 'Wu, Tianfu', 'Yang, Xingming', 'Zhang, Luming', 'Wu, Kewei']\n",
      "['Xu, Dezhong', 'Fu, Heng', 'Wu, Lifang', 'Jian, Meng', 'Wang, Dong', 'Liu, Xu']\n",
      "['Yan, Rui', 'Tang, Jinhui', 'Shu, Xiangbo', 'Li, Zechao', 'Tian, Qi', 'Acm']\n"
     ]
    }
   ],
   "source": [
    "from pybtex.database import parse_file\n",
    "\n",
    "bib_data = parse_file('temp.bib')\n",
    "\n",
    "\n",
    "temp = [bib_data.entries[i] for i in list(bib_data.entries)]\n",
    "#temp = [i  for i in list(bib_data.entries)]\n",
    "\n",
    "#print(temp)\n",
    "for i in temp :\n",
    "    authors = [str(j) for j in list(i.persons['author'])]\n",
    "    print(authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     \n",
    "import re\n",
    "\n",
    "DOI_List = []\n",
    "\n",
    "BibTex_File = open('temp.bib', 'r') \n",
    "\n",
    "for BibTex_Line in BibTex_File:\n",
    "    if BibTex_Line[:6] == '   DOI' :\n",
    "        DOI_Search = re.search('{(.+?)}', BibTex_Line)\n",
    "        if DOI_Search: DOI_List.append(DOI_Search.group(1))\n",
    "        \n",
    "BibTex_File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Loop through the list and store the citation data from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract', 'arxivId', 'authors', 'citationVelocity', 'citations', 'corpusId', 'doi', 'fieldsOfStudy', 'influentialCitationCount', 'isOpenAccess', 'isPublisherLicensed', 'is_open_access', 'is_publisher_licensed', 'numCitedBy', 'numCiting', 'paperId', 'references', 'title', 'topics', 'url', 'venue', 'year'])\n"
     ]
    }
   ],
   "source": [
    "import semanticscholar as sch\n",
    "\n",
    "papers = []\n",
    "\n",
    "for DOI in DOI_List:\n",
    "    paper = sch.paper(DOI, timeout=2)\n",
    "    papers.append(paper)\n",
    "\n",
    "print(papers[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Loop through the resulting citation data and produce a table\n",
    "- [ ] Need to handle case where schemantics scholar returns null as a aritcle cannot be found (eg recent publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Author</th><th>Year</th><th>Title</th><th>Citation Count</th><tr><td>A. B. Abkenar</td><td>2016</td><td>Energy Considerations for Continuous Group Activity Recognition Using Mobile Devices: The Case of GroupSense</td><td>9</td></tr><tr><td colspan=\"4\">Human activity recognition using mobile sensors is becoming increasingly important. Scaling up from individuals to groups, that is, Group Activity Recognition (GAR), has attracted significant attention recently. This paper investigates energy consumption for GAR and proposes a novel distributed middleware called GroupSense for mobile GAR. We implemented and tested GroupSense, which incorporates a protocol for the exchange of information required for GAR. We also investigated the battery drain of continuous activity recognition in a range of simple GAR scenarios. We then conclude with lessons learnt for GAR.</td></tr><tr><td>A. B. Abkenar</td><td>2019</td><td>GARSAaaS: group activity recognition and situation analysis as a service</td><td>3</td></tr><tr><td colspan=\"4\">Human activity recognition using embedded mobile and embedded sensors is becoming increasingly important. Scaling up from individuals to groups, that is, group activity recognition, has attracted significant attention recently. This paper proposes a model and specification language for group activities called GroupSense-L, and a novel architecture called GARSAaaS (GARSA-as-a-Service) to provide services for mobile Group Activity Recognition and Situation Analysis (or GARSA) applications. We implemented and evaluated GARSAaaS which is an extension of a framework called GroupSense (Abkenar et al., 2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA), 2016) where sensor data, collected using smartphone sensors, smartwatch sensors and embedded sensors in things, are aggregated via a protocol for these different devices to share information, as required for GARSA. We illustrate our approach via a scenario for providing services for bush walking leaders and bush walkers in a bushwalking group activity. We demonstrate the feasibility of our model and expressiveness of our proposed model.</td></tr><tr><td>A. B. Abkenar</td><td>2019</td><td>GroupSense</td><td>0</td></tr><tr><td colspan=\"4\">Human activity recognition using embedded mobile and embedded sensors is becoming increasingly important. Scaling up from individuals to groups, that is, Group Activity Recognition (GAR), has attracted significant attention recently. This article proposes a model and modeling language for GAR called GroupSense-L and a novel distributed middleware called GroupSense for mobile GAR. We implemented and tested GroupSense using smartphone sensors, smartwatch sensors, and embedded sensors in things, where we have a protocol for these different devices to exchange information required for GAR. A range of continuous group activities (from simple to fairly complex) illustrates our approach and demonstrates the feasibility of our model and richness of the proposed specialization. We then conclude with lessons learned for GAR and future work.</td></tr><tr><td>A. Akar</td><td>2019</td><td>Mask Guided Fusion for Group Activity Recognition in Images</td><td>1</td></tr><tr><td colspan=\"4\">Recognizing group activities from still images is a challenging problem since images lack motion and temporal information that makes it easier to differentiate foreground from background. Nevertheless, images present rich spatial content that can be effectively leveraged for better feature representation and recognition. In this paper, we propose a two-stream convolutional neural network approach for group activity recognition. Our proposed approach is based on using person segment mask images to guide feature learning process. Our method is capable of inferring group relations without the need of bottom-up approaches and low-level annotations. To this end, we utilize three ways of fusing RGB and person segment mask feature maps. Experimental results demonstrate that person mask guidance provides a complementary learning process by outperforming previous methods with a large margin.</td></tr><tr><td>Mohammed Al-habib</td><td>2019</td><td>Cooperative Hierarchical Framework for Group Activity Recognition: From Group Detection to Multi-activity Recognition</td><td>1</td></tr><tr><td colspan=\"4\">Deep neural network algorithms have shown promising performance for many tasks in computer vision field. Several neural network-based methods have been proposed to recognize group activities from video sequences. However, there are still several challenges that are related to multiple groups with different activities within a scene. The strong correlation that exists among individual motion, groups and activities can be utilized to detect groups and recognize their concurrent activities. Motivated by these observations, we propose a unified deep learning framework for detecting multiple groups and recognizing their corresponding collective activity based on Long Short-Term Memory (LSTM) network. In this framework, we use a pre-trained convolutional neural network (CNN) to extract features from the frames and appearances of persons. An objective function has been proposed to learn the amount of pairwise interaction between persons. The obtained individual features are passed to a clustering algorithm to detect groups in the scene. Then, an LSTM based model is used to recognize group activities. Together with this, a scene level CNN followed by LSTM is used to extract and learn scene level feature. Finally, the activities from the group level and the scene context level are integrated to infer the collective activity. The proposed method is evaluated on the benchmark collective activity dataset and compared with several baselines. The experimental results show its competitive performance for the collective activity recognition task.</td></tr><tr><td>S. Azar</td><td>2019</td><td>Convolutional Relational Machine for Group Activity Recognition</td><td>24</td></tr><tr><td colspan=\"4\">We present an end-to-end deep Convolutional Neural Network called Convolutional Relational Machine (CRM) for recognizing group activities that utilizes the information in spatial relations between individual persons in image or video. It learns to produce an intermediate spatial representation (activity map) based on individual and group activities. A multi-stage refinement component is responsible for decreasing the incorrect predictions in the activity map. Finally, an aggregation component uses the refined information to recognize group activities. Experimental results demonstrate the constructive contribution of the information extracted and represented in the form of the activity map. CRM shows advantages over state-of-the-art models on Volleyball and Collective Activity datasets.</td></tr><tr><td>J. López</td><td>2016</td><td>Constrained self-organizing feature map to preserve feature extraction topology</td><td>2</td></tr><tr><td colspan=\"4\">In many classification problems, it is necessary to consider the specific location of an n-dimensional space from which features have been calculated. For example, considering the location of features extracted from specific areas of a two-dimensional space, as an image, could improve the understanding of a scene for a video surveillance system. In the same way, the same features extracted from different locations could mean different actions for a 3D HCI system. In this paper, we present a self-organizing feature map able to preserve the topology of locations of an n-dimensional space in which the vector of features have been extracted. The main contribution is to implicitly preserving the topology of the original space because considering the locations of the extracted features and their topology could ease the solution to certain problems. Specifically, the paper proposes the n-dimensional constrained self-organizing map preserving the input topology (nD-SOM-PINT). Features in adjacent areas of the n-dimensional space, used to extract the feature vectors, are explicitly in adjacent areas of the nD-SOM-PINT constraining the neural network structure and learning. As a study case, the neural network has been instantiate to represent and classify features as trajectories extracted from a sequence of images into a high level of semantic understanding. Experiments have been thoroughly carried out using the CAVIAR datasets (Corridor, Frontal and Inria) taken into account the global behaviour of an individual in order to validate the ability to preserve the topology of the two-dimensional space to obtain high-performance classification for trajectory classification in contrast of non-considering the location of features. Moreover, a brief example has been included to focus on validate the nD-SOM-PINT proposal in other domain than the individual trajectory. Results confirm the high accuracy of the nD-SOM-PINT outperforming previous methods aimed to classify the same datasets.</td></tr><tr><td>Amin Bakhshandehabkenar</td><td>2014</td><td>A Framework for Continuous Group Activity Recognition Using Mobile Devices: Concept and Experimentation</td><td>6</td></tr><tr><td colspan=\"4\">Group Activity Recognition (GAR) is a challenging research area in context-aware computing which has attracted much attention recently. Many studies have been conducted in the field of activity recognition (AR) along with their applications in domains such as health, smart homes, daily living and life logging. However, still many open issues exist. Lack of an energy-efficient approach is one of the most vital issues in the context of AR. GAR work often suffers from energy consumption issues for the reason that, apart from AR process, there is the requirement to have more interaction among members of the group and a need to run more complex recognition processes. Moreover, almost all work in GAR are technology-oriented and assume that our real-life environment remains fixed once the system has been established, but this may not be the case. Hence, we propose a framework called Group Sense for GAR towards addressing these issues. Also, a relatively simple scheme for GAR, with a protocol for the exchange of information required for GAR, has been implemented, tested and evaluated. We then conclude with lessons learnt for GAR.</td></tr><tr><td>Amine Lotfi Bourbia</td><td>2016</td><td>Temporal Dependency Rule Learning Based Group Activity Recognition in Smart Spaces</td><td>8</td></tr><tr><td colspan=\"4\">We present a generic framework for group activity recognition using simple non-obtrusive sensors. The proposed scheme is based on that group activity patterns can be derived from mining interval-based relationships between users' temporally overlapped actions. We leverage a hybrid architecture of probabilistic and logic knowledge that can capture the essence of the temporal dependencies, represented as a set of weighed rules. It can also learn different weights for common rules between similar group activities, which share most of sensor events and events order. The evaluation results show that our scheme outperforms the sequential baseline model, a mixture of Gaussian Hidden Markov Models.</td></tr><tr><td>Jiannan Cai</td><td>2019</td><td>Two-step long short-term memory method for identifying construction activities through positional and attentional cues</td><td>17</td></tr><tr><td colspan=\"4\">Abstract Recognizing construction activities and involved working groups is critical to enhancing construction safety and improving productivity. Most existing studies use videos that only contain one activity with involved entities and rely solely on the spatial-temporal relationship among entities. However, in practice, many workers and machines co-exist and collaborate to accomplish different activities, and not all of them are relevant to the same activity, even though they are spatially close. This paper presents a two-step classification approach – working group identification followed by activity recognition, leveraging both positional and attentional cues, to recognize complex interactions and their involved entities from videos that contain different activities with multiple entities. The spatial and attentional states of individual entities are represented numerically, and the corresponding positional and attentional cues between two entities are computed. Long short-term memory (LSTM) networks are designed to (1) classify whether two entities belong to the same group, and (2) recognize the activities they are involved in. The newly created method is validated using two sets of construction videos. Identifying working groups before recognizing ongoing activities enables the exclusion of group-irrelevant entities and thus, improves the performance. Moreover, by leveraging both positional and attentional cues, the accuracy increases from 85% to 95% compared with cases using positional cues alone.</td></tr><tr><td>B. Chen</td><td>2019</td><td>Group Activity Recognition to Support Collaboration in Creative Digital Space</td><td>0</td></tr><tr><td colspan=\"4\">In order to orchestrate collaborative group work, such as a brainstorming workshop or active learning class, a facilitator must monitor all groups simultaneously. In this paper, we propose a method for recognizing group activities, based on machine learning, where the input features are derived from the analysis of raw system events generated by a collaboration system known as \"creative digital space\" that we developed before. To verify the effectiveness of the proposed method, we also evaluated the results obtained by using the test data collected from active learning classes.</td></tr><tr><td>Nam-Gyu Cho</td><td>2015</td><td>Group Activity Recognition with Group Interaction Zone Based on Relative Distance Between Human Objects</td><td>30</td></tr><tr><td colspan=\"4\">In this paper, we address the problem of recognizing group activities of human objects based on their motion trajectory analysis. In order to resolve the complexity and ambiguity problems caused by a large number of human objects, we propose a Group Interaction Zone (GIZ) to detect meaningful groups in a scene to effectively handle noisy information. Two novel features, Group Interaction Energy (GIE) feature and Attraction and Repulsion Features, are proposed to better describe group activities within a GIZ. We demonstrate the performance of our method in two ways by (i) comparing the performance of the proposed method with the previous methods and (ii) analyzing the influence of the proposed features and GIZ-based meaningful group detection on group activity recognition using public datasets.</td></tr><tr><td>Zhiwei Deng</td><td>2016</td><td>Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</td><td>163</td></tr><tr><td colspan=\"4\">Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.</td></tr><tr><td>V. Elangovan</td><td>2012</td><td>Team activity analysis and recognition based on Kinect depth map and optical imagery techniques</td><td>8</td></tr><tr><td colspan=\"4\">Kinect cameras produce low-cost depth map video streams applicable for conventional surveillance systems. However, commonly applied image processing techniques are not directly applicable for depth map video processing. Kinect depth map images contain range measurement of objects at expense of having spatial features of objects suppressed. For example, typical objects' attributes such as textures, color tones, intensity, and other characteristic attributes cannot be fully realized by processing depth map imagery. In this paper, we demonstrate application of Kinect depth map and optical imagery for characterization of indoor and outdoor group activities. A Casual-Events State Inference (CESI) technique is proposed for spatiotemporal recognition and reasoning of group activities. CESI uses an ontological scheme for representation of casual distinctiveness of a priori known group activities. By tracking and serializing distinctive atomic group activities, CESI allows discovery of more complex group activities. A Modified Sequential Hidden Markov Model (MS-HMM) is implemented for trail analysis of atomic events representing correlated group activities. CESI reasons about five levels of group activities including: Merging, Planning, Cooperation, Coordination, and Dispersion. In this paper, we present results of capability of CESI approach for characterization of group activities taking place both in indoor and outdoor. Based on spatiotemporal pattern matching of atomic activities representing a known group activities, the CESI is able to discriminate suspicious group activity from normal activities. This paper also presents technical details of imagery techniques implemented for detection, tracking, and characterization of atomic events based on Kinect depth map and optical imagery data sets. Various experimental scenarios in indoors and outdoors (e.g. loading and unloading of objects, human-vehicle interactions etc.,) are carried to demonstrate effectiveness and efficiency of the proposed model for characterization of distinctive group activities.</td></tr><tr><td>V. Elangovan</td><td>2013</td><td>A robust technique for semantic annotation of group activities based on recognition of extracted features in video streams</td><td>5</td></tr><tr><td colspan=\"4\">Recognition and understanding of group activities can significantly improve situational awareness in Surveillance Systems. To maximize reliability and effectiveness of Persistent Surveillance Systems, annotations of sequential images gathered from video streams (i.e. imagery and acoustic features) must be fused together to generate semantic messages describing group activities (GA). To facilitate efficient fusion of extracted features from any physical sensors a common structure will suffice to ease integration of processed data into new comprehension. In this paper, we describe a framework for extraction and management of pertinent features/attributes vital for annotation of group activities reliably. A robust technique is proposed for fusion of generated events and entities’ attributes from video streams. A modified Transducer Markup Language (TML) is introduced for semantic annotation of events and entities attributes. By aggregation of multi-attribute TML messages, we have demonstrated that salient group activities can be spatiotemporal can be reliable annotated. This paper discusses our experimental results; our analysis of a set of simulated group activities performed under different contexts and demonstrates the efficiency and effectiveness of the proposed modified TML data structure which facilitates seamless fusion of extracted information from video streams.</td></tr><tr><td>V. Elangovan</td><td>2014</td><td>Knowledge discovery in group activities through sequential observation analysis</td><td>5</td></tr><tr><td colspan=\"4\">Understanding of Group Activities (GA) has significant applications in civilian and military domains. The process of understanding GA is typically involved with spatiotemporal analysis of multi-modality sensor data. Video imagery is one popular sensing modality that offers rich data, however, data associated with imagery source may become fragmented and discontinued due to a number of reasons (e.g., data transmission, or observation obstructions and occlusions). However, making sense out of video imagery is a real challenge. It requires a proper inference working model capable of analyzing video imagery frame by frame, extract and inference spatiotemporal information pertaining to observations while developing an incremental perception of the GA as they emerge overtime. In this paper, we propose an ontology based GA recognition where three inference Hidden Markov Models (HMM’s) are used for predicting group activities taking place in outdoor environments and different task operational taxonomy. The three competing models include: a concatenated HMM, a cascaded HMM, and a context-based HMM. The proposed ontology based GA-HMM was subjected to set of semantically annotated visual observations from outdoor group activity experiments. Experimental results from GA-HMM are presented with technical discussions on design of each model and their potential implication to Persistent Surveillance Systems (PSS).</td></tr><tr><td>Chairani Fauzi</td><td>2019</td><td>Group Activity Recognition Method based on Camera in The Building</td><td>0</td></tr><tr><td colspan=\"4\">Abstract Group activities are an activity carried out together at the same time and place. In the office/university environment, they play a role in energy consumption. For this reason, the smart building needs group activity recognition (GAR) systems for electrical device control. Several studies on the GAR have been carried out in the outdoor environment, such as exercising, walking, or running. However, indoor group activities that share of energy consumption are found still rare, such as in meetings, seminars, and classroom activities. This study proposes a GAR method in the buildings using multi-image based on Camera through the sitting position of people and formed from the face identified from the image and visualized into the mapping. The simulation was carried out based on references from the scenario of the meeting, seminar, and class activity in the S307-room, Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada. The Neural Network algorithm was used to the GAR. The performance was evaluated in various measures such as precision, accuracy, and recall. The result of the GAR accuracy of the learning phase was 93.33%. The results of the GAR accuracy of testing phase were 63% and the error of GAR were 37%, respectively.</td></tr><tr><td>G. Florea</td><td>2020</td><td>Multimodal Deep Learning for Group Activity Recognition in Smart Office Environments</td><td>1</td></tr><tr><td colspan=\"4\">Deep learning (DL) models have emerged in recent years as the state-of-the-art technique across numerous machine learning application domains. In particular, image processing-related tasks have seen a significant improvement in terms of performance due to increased availability of large datasets and extensive growth of computing power. In this paper we investigate the problem of group activity recognition in office environments using a multimodal deep learning approach, by fusing audio and visual data from video. Group activity recognition is a complex classification task, given that it extends beyond identifying the activities of individuals, by focusing on the combinations of activities and the interactions between them. The proposed fusion network was trained based on the audio–visual stream from the AMI Corpus dataset. The procedure consists of two steps. First, we extract a joint audio–visual feature representation for activity recognition, and second, we account for the temporal dependencies in the video in order to complete the classification task. We provide a comprehensive set of experimental results showing that our proposed multimodal deep network architecture outperforms previous approaches, which have been designed for unimodal analysis, on the aforementioned AMI dataset.</td></tr><tr><td>Harshala Gammulle</td><td>2018</td><td>Multi-Level Sequence GAN for Group Activity Recognition</td><td>14</td></tr><tr><td colspan=\"4\">We propose a novel semi supervised, Multi Level Sequential Generative Adversarial Network (MLS-GAN) architecture for group activity recognition. In contrast to previous works which utilise manually annotated individual human action predictions, we allow the models to learn it’s own internal representations to discover pertinent sub-activities that aid the final group activity recognition task. The generator is fed with person-level and scene-level features that are mapped temporally through LSTM networks. Action-based feature fusion is performed through novel gated fusion units that are able to consider long-term dependancies, exploring the relationships among all individual actions, to learn an intermediate representation or ‘action code’ for the current group activity. The network achieves it’s semi-supervised behaviour by allowing it to perform group action classification together with the adversarial real/fake validation. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed architecture. Furthermore, we show that utilising both person-level and scene-level features facilitates the group activity prediction better than using only person-level features. Our proposed architecture outperforms current state-of-the-art results for sports and pedestrian based classification tasks on Volleyball and Collective Activity datasets, showing it’s flexible nature for effective learning of group activities (This research was supported by the Australian Research Council’s Linkage Project LP140100282 “Improving Productivity and Efficiency of Australian Airports”).</td></tr><tr><td>S. Gong</td><td>2003</td><td>Recognition of group activities using dynamic probabilistic networks</td><td>300</td></tr><tr><td colspan=\"4\">Dynamic Probabilistic Networks (DPNs) are exploited for modeling the temporal relationships among a set of different object temporal events in the scene for a coherent and robust scene-level behaviour interpretation. In particular, we develop a Dynamically Multi-Linked Hidden Markov Model (DML-HMM) to interpret group activities involving multiple objects captured in an outdoor scene. The model is based on the discovery of salient dynamic interlinks among multiple temporal events using DPNs. Object temporal events are detected and labeled using Gaussian Mixture Models with automatic model order selection. A DML-HMM is built using Schwarz's Bayesian Information Criterion based factorisation resulting in its topology being intrinsically determined by the underlying causality and temporal order among different object events. Our experiments demonstrate that its performance on modelling group activities in a noisy outdoor scene is superior compared to that of a Multi-Observation Hidden Markov Model (MOHMM), a Parallel Hidden Markov Model (PaHMM) and a Coupled Hidden Markov Model (CHMM).</td></tr><tr><td>D. Gordon</td><td>2013</td><td>Towards Collaborative Group Activity Recognition Using Mobile Devices</td><td>40</td></tr><tr><td colspan=\"4\">In this paper, we present a novel approach for distributed recognition of collaborative group activities using only mobile devices and their sensors. Information must be exchanged between nodes for effective group activity recognition (GAR). Here we investigated the effects of exchanging that information at different data abstraction levels with respect to recognition rates, power consumption, and wireless communication volumes. The goal is to identify the tradeoff between energy consumption and recognition accuracy for GAR problems. For the given set of activities, using locally extracted features for global, group activity recognition is advantageous as energy consumption was reduced by 10 % without experiencing any significant loss in recognition rates. Using locally classified single-user activities, however, caused a 47 % loss in recognition capabilities, making this approach unattractive. Local clustering proved to be effective for recognizing group activities, by greatly reducing power consumption while incurring a loss of only 2.8 % in recognition accuracy.</td></tr><tr><td>Hossein Hajimirsadeghi</td><td>2017</td><td>Multi-Instance Classification by Max-Margin Training of Cardinality-Based Markov Networks</td><td>12</td></tr><tr><td colspan=\"4\">We propose a probabilistic graphical framework for multi-instance learning (MIL) based on Markov networks. This framework can deal with different levels of labeling ambiguity (i.e., the portion of positive instances in a bag) in weakly supervised data by parameterizing cardinality potential functions. Consequently, it can be used to encode different cardinality-based multi-instance assumptions, ranging from the standard MIL assumption to more general assumptions. In addition, this framework can be efficiently used for both binary and multiclass classification. To this end, an efficient inference algorithm and a discriminative latent max-margin learning algorithm are introduced to train and test the proposed multi-instance Markov network models. We evaluate the performance of the proposed framework on binary and multi-class MIL benchmark datasets as well as two challenging computer vision tasks: cyclist helmet recognition and human group activity recognition. Experimental results verify that encoding the degree of ambiguity in data can improve classification performance.</td></tr><tr><td>Hossein Hajimirsadeghi</td><td>2015</td><td>Learning Ensembles of Potential Functions for Structured Prediction with Latent Variables</td><td>9</td></tr><tr><td colspan=\"4\">Many visual recognition tasks involve modeling variables which are structurally related. Hidden conditional random fields (HCRFs) are a powerful class of models for encoding structure in weakly supervised training examples. This paper presents HCRF-Boost, a novel and general framework for learning HCRFs in functional space. An algorithm is proposed to learn the potential functions of an HCRF as a combination of abstract nonlinear feature functions, expressed by regression models. Consequently, the resulting latent structured model is not restricted to traditional log-linear potential functions or any explicit parameterization. Further, functional optimization helps to avoid direct interactions with the possibly large parameter space of nonlinear models and improves efficiency. As a result, a complex and flexible ensemble method is achieved for structured prediction which can be successfully used in a variety of applications. We validate the effectiveness of this method on tasks such as group activity recognition, human action recognition, and multi-instance learning of video events.</td></tr><tr><td>Mostafa S. Ibrahim</td><td>2018</td><td>Hierarchical Relational Networks for Group Activity Recognition and Retrieval</td><td>43</td></tr><tr><td colspan=\"4\">Modeling structured relationships between people in a scene is an important step toward visual understanding. We present a Hierarchical Relational Network that computes relational representations of people, given graph structures describing potential interactions. Each relational layer is fed individual person representations and a potential relationship graph. Relational representations of each person are created based on their connections in this particular graph. We demonstrate the efficacy of this model by applying it in both supervised and unsupervised learning paradigms. First, given a video sequence of people doing a collective activity, the relational scene representation is utilized for multi-person activity recognition. Second, we propose a Relational Autoencoder model for unsupervised learning of features for action and scene retrieval. Finally, a Denoising Autoencoder variant is presented to infer missing people in the scene from their context. Empirical results demonstrate that this approach learns relational feature representations that can effectively discriminate person and group activity classes.</td></tr><tr><td>Mostafa S. Ibrahim</td><td>2016</td><td>A Hierarchical Deep Temporal Model for Group Activity Recognition</td><td>234</td></tr><tr><td colspan=\"4\">In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. To make use of these observations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of individual people in a sequence and another LSTM model is designed to aggregate person-level information for whole activity understanding. We evaluate our model over two datasets: the Collective Activity Dataset and a new volleyball dataset. Experimental results demonstrate that our proposed model improves group activity recognition performance compared to baseline methods.</td></tr><tr><td>Jaeyong Ju</td><td>2015</td><td>Recognition of Human Group Activity for Video Analytics</td><td>1</td></tr><tr><td colspan=\"4\">Human activity recognition is an important and challenging task for video content analysis and understanding. Individual activity recognition has been well studied recently. However, recognizing the activities of human group with more than three people having complex interactions is still a formidable challenge. In this paper, a novel human group activity recognition method is proposed to deal with complex situation where there are multiple sub-groups. To characterize the inherent interactions of intra-subgroups and inter-subgroups with the varying number of participants, this paper proposes three types of group-activity descriptor using motion trajectory and appearance information of people. Experimental results on a public human group activity dataset demonstrate effectiveness of the proposed method.</td></tr><tr><td>Pil-Soo Kim</td><td>2018</td><td>Discriminative context learning with gated recurrent unit for group activity recognition</td><td>30</td></tr><tr><td colspan=\"4\">Abstract In this study, we address the problem of similar local motions that create confusion within different group activities. To reduce the influences of motions, we propose a discriminative group context feature (DGCF) that considers prominent sub-events. Moreover, we adopt a gated recurrent unit (GRU) model that can learn temporal changes in a sequence. In real-world scenarios, people perform activities with different temporal lengths. The GRU model handles an arbitrary length of data for training with nonlinear hidden units in the network. However, when we use a deep neural network model, data scarcity causes overfitting problems. Data augmentation methods for images are ineffective for trajectory data augmentation. Thus, we also propose a method for trajectory augmentation. We evaluate the effectiveness of the proposed method on three datasets. In our experiments on each dataset, we show that the proposed method outperforms the competing state-of-the-art methods for group activity recognition.</td></tr><tr><td>Young-Ji Kim</td><td>2014</td><td>Group Activity Recognition with Group Interaction Zone</td><td>15</td></tr><tr><td colspan=\"4\">In this paper, we address the problem of recognizing group activities that include interactions between human objects based on their motion trajectory analysis. In order to resolve the complexity and ambiguity problems caused by a large number of human objects, we propose a Group Interaction Zone (GIZ) to detect meaningful groups in a scene so as to be robust against noisy information. Two novel features, Group Interaction Energy feature and Attraction and Repulsion Features, are proposed to better describe group activities within a GIZ. We demonstrate the effectiveness of our method with other methods on the public BEHAVE dataset.</td></tr><tr><td>Tian Lan</td><td>2012</td><td>Discriminative Latent Models for Recognizing Contextual Group Activities</td><td>241</td></tr><tr><td colspan=\"4\">In this paper, we go beyond recognizing the actions of individuals and focus on group activities. This is motivated from the observation that human actions are rarely performed in isolation; the contextual information of what other people in the scene are doing provides a useful cue for understanding high-level activities. We propose a novel framework for recognizing group activities which jointly captures the group activity, the individual person actions, and the interactions among them. Two types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. In particular, we propose three different approaches to model the person-person interaction. One approach is to explore the structures of person-person interaction. Differently from most of the previous latent structured models, which assume a predefined structure for the hidden layer, e.g., a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. The second approach explores person-person interaction in the feature level. We introduce a new feature representation called the action context (AC) descriptor. The AC descriptor encodes information about not only the action of an individual person in the video, but also the behavior of other people nearby. The third approach combines the above two. Our experimental results demonstrate the benefit of using contextual information for disambiguating group activities.</td></tr><tr><td>Stéphane Lathuilière</td><td>2017</td><td>Recognition of Group Activities in Videos Based on Single-and Two-Person Descriptors</td><td>7</td></tr><tr><td colspan=\"4\">Group activity recognition from videos is a very challenging problem that has barely been addressed. We propose an activity recognition method using group context. In order to encode both single-person description and two-person interactions, we learn mappings from highdimensional feature spaces to low-dimensional dictionaries. In particular the proposed two-person descriptor takes into account geometric characteristics of the relative pose and motion between the two persons. Both single-person and two-person representations are then used to define unary and pairwise potentials of an energy function, whose optimization leads to the structured labeling of persons involved in the same activity. An interesting feature of the proposed method is that, unlike the vast majority of existing methods, it is able to recognize multiple distinct group activities occurring simultaneously in a video. The proposed method is evaluated with datasets widely used for group activity recognition, and is compared with several baseline methods.</td></tr><tr><td>Ruonan Li</td><td>2012</td><td>Recognizing Interactive Group Activities Using Temporal Interaction Matrices and Their Riemannian Statistics</td><td>19</td></tr><tr><td colspan=\"4\">While video-based activity analysis and recognition has received much attention, a large body of existing work deals with activities of a single subject. Modeling and recognition of coordinated multi-subject activities, or group activities, present in a variety of applications such as surveillance, sports, and biological monitoring records, etc., is the main objective of this paper. Unlike earlier attempts which model the complex spatial temporal constraints among multiple subjects with a parametric Bayesian network, we propose a compact and discriminative descriptor referred to as the Temporal Interaction Matrix for representing a coordinated group motion pattern. Moreover, we characterize the space of the Temporal Interaction Matrices using the Discriminative Temporal Interaction Manifold (DTIM), and use it as a framework within which we develop a data-driven strategy to characterize the group motion pattern without employing specific domain knowledge. In particular, we establish probability densities on the DTIM for compactly describing the statistical properties of the coordinations and interactions among multiple subjects in a group activity. For each class of group activity, we learn a multi-modal density function on the DTIM. A Maximum a Posteriori (MAP) classifier on the manifold is then designed for recognizing new activities. In addition, we have extended this model to one with which we can explicitly distinguish the participants from non-participants. We demonstrate how the framework can be applied to motions represented by point trajectories as well as articulated human actions represented by images. Experiments on both cases show the effectiveness of the proposed approach.</td></tr><tr><td>Xin Li</td><td>2017</td><td>SBGAR: Semantics Based Group Activity Recognition</td><td>43</td></tr><tr><td colspan=\"4\">Activity recognition has become an important function in many emerging computer vision applications e.g. automatic video surveillance system, human-computer interaction application, and video recommendation system, etc. In this paper, we propose a novel semantics based group activity recognition scheme, namely SBGAR, which achieves higher accuracy and efficiency than existing group activity recognition methods. SBGAR consists of two stages: in stage I, we use a LSTM model to generate a caption for each video frame; in stage II, another LSTM model is trained to predict the final activity categories based on these generated captions. We evaluate SBGAR using two well-known datasets: the Collective Activity Dataset and the Volleyball Dataset. Our experimental results show that SBGAR improves the group activity recognition accuracy with shorter computation time compared to the state-of-the-art methods.</td></tr><tr><td>Weiyao Lin</td><td>2014</td><td>A New Network-Based Algorithm for Human Activity Recognition in Videos</td><td>17</td></tr><tr><td colspan=\"4\">In this paper, a new network-transmission-based (NTB) algorithm is proposed for human activity recognition in videos. The proposed NTB algorithm models the entire scene as an error-free network. In this network, each node corresponds to a patch of the scene and each edge represents the activity correlation between the corresponding patches. Based on this network, we further model people in the scene as packages, while human activities can be modeled as the process of package transmission in the network. By analyzing these specific package transmission processes, various activities can be effectively detected. The implementation of our NTB algorithm into abnormal activity detection and group activity recognition are described in detail in this paper. Experimental results demonstrate the effectiveness of our proposed algorithm.</td></tr><tr><td>Weiyao Lin</td><td>2013</td><td>A Heat-Map-Based Algorithm for Recognizing Group Activities in Videos</td><td>39</td></tr><tr><td colspan=\"4\">In this paper, a new heat-map-based algorithm is proposed for group activity recognition. The proposed algorithm first models human trajectories as series of heat sources and then applies a thermal diffusion process to create a heat map (HM) for representing the group activities. Based on this HM, a new key-point-based (KPB) method is used for handling the alignments among HMs with different scales and rotations. A surface-fitting (SF) method is also proposed for recognizing group activities. Our proposed HM feature can efficiently embed the temporal motion information of the group activities while the proposed KPB and SF methods can effectively utilize the characteristics of the HM for activity recognition. Section IV demonstrates the effectiveness of our proposed algorithms.</td></tr><tr><td>Jichao Liu</td><td>2019</td><td>Deep Fully Connected Model for Collective Activity Recognition</td><td>5</td></tr><tr><td colspan=\"4\">Group activity recognition is a challenging task because there is an exponentially large number of semantic and geometrical relationships among individuals. This makes it difficult to model these interactions and merge them as a whole for group activity classification. In this paper, we propose a deep fully-connected model for group recognition, first we use the spatial-temporal model based on convolution neural network (CNN) and long short-term memory networks (LSTM) network to capture the dynamic features of each person. Then, we use the fully-connected conditional random field (FCCRF) to learn the interactions between people. Finally, with FCCRF potential functions we re-fine the activity recognition predicted by the spatial-temporal model. The experimental results on collective activity data-set and collective activity extended data-set show that our model improves recognition accuracy over baseline methods and gets competitive results in comparison to the state-of-the-art models.</td></tr><tr><td>Moin Nabi</td><td>2013</td><td>Temporal Poselets for Collective Activity Detection and Recognition</td><td>20</td></tr><tr><td colspan=\"4\">Detection and recognition of collective human activities are important modules of any system devoted to high level social behavior analysis. In this paper, we present a novel semantic-based spatio-temporal descriptor which can cope with several interacting people at different scales and multiple activities in a video. Our descriptor is suitable for modelling the human motion interaction in crowded environments - the scenario most difficult to analyse because of occlusions. In particular, we extend the Pose let detector approach by defining a descriptor based on Pose let activation patterns over time, named TPOS. We will show that this descriptor can effectively tackle complex real scenarios allowing to detect humans in the scene, to localize (in space-time) human activities, and perform collective group activity recognition in a joint manner, reaching state-of-the-art results.</td></tr><tr><td>Azin Poshtkar</td><td>2015</td><td>Physical environment virtualization for human activities recognition</td><td>6</td></tr><tr><td colspan=\"4\">Human activity recognition research relies heavily on extensive datasets to verify and validate performance of activity recognition algorithms. However, obtaining real datasets are expensive and highly time consuming. A physics-based virtual simulation can accelerate the development of context based human activity recognition algorithms and techniques by generating relevant training and testing videos simulating diverse operational scenarios. In this paper, we discuss in detail the requisite capabilities of a virtual environment to aid as a test bed for evaluating and enhancing activity recognition algorithms. To demonstrate the numerous advantages of virtual environment development, a newly developed virtual environment simulation modeling (VESM) environment is presented here to generate calibrated multisource imagery datasets suitable for development and testing of recognition algorithms for context-based human activities. The VESM environment serves as a versatile test bed to generate a vast amount of realistic data for training and testing of sensor processing algorithms. To demonstrate the effectiveness of VESM environment, we present various simulated scenarios and processed results to infer proper semantic annotations from the high fidelity imagery data for human-vehicle activity recognition under different operational contexts.</td></tr><tr><td>Mengshi Qi</td><td>2018</td><td>stagNet: An Attentive Semantic RNN for Group Activity Recognition</td><td>44</td></tr><tr><td colspan=\"4\">Group activity recognition plays a fundamental role in a variety of applications, e.g. sports video analysis and intelligent surveillance. How to model the spatio-temporal contextual information in a scene still remains a crucial yet challenging issue. We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph. A semantic graph is explicitly modeled to describe the spatial context of the whole scene, which is further integrated with the temporal factor via structural-RNN. Benefiting from the ‘factor sharing’ and ‘message passing’ mechanisms, our model is capable of extracting discriminative spatio-temporal features and capturing inter-group relationships. Moreover, we adopt a spatio-temporal attention model to attend to key persons/frames for improved performance. Two widely-used datasets are employed for performance evaluation, and the extensive results demonstrate the superiority of our method.</td></tr><tr><td>Mengshi Qi</td><td>2020</td><td>stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition</td><td>22</td></tr><tr><td colspan=\"4\">In real life, group activity recognition plays a significant and fundamental role in a variety of applications, e.g. sports video analysis, abnormal behavior detection, and intelligent surveillance. In a complex dynamic scene, a crucial yet challenging issue is how to better model the spatio-temporal contextual information and inter-person relationship. In this paper, we present a novel attentive semantic recurrent neural network (RNN), namely, stagNet, for understanding group activities and individual actions in videos, by combining the spatio-temporal attention mechanism and semantic graph modeling. Specifically, a structured semantic graph is explicitly modeled to express the spatial contextual content of the whole scene, which is further incorporated with the temporal factor through structural-RNN. By virtue of the “factor sharing” and “message passing” mechanisms, our stagNet is capable of extracting discriminative and informative spatio-temporal representations and capturing inter-person relationships. Moreover, we adopt a spatio-temporal attention model to focus on key persons/frames for improved recognition performance. Besides, a body-region attention and a global-part feature pooling strategy are devised for individual action recognition. In experiments, four widely-used public datasets are adopted for performance evaluation, and the extensive results demonstrate the superiority and effectiveness of our method.</td></tr><tr><td>Silvia Rossi</td><td>2020</td><td>Working together: a DBN approach for individual and group activity recognition</td><td>2</td></tr><tr><td colspan=\"4\">Human activity recognition is gaining more and more the attention of researchers due to its applicability in many different fields such as health monitoring, smart environments, etc. Activity recognition solutions typically focus on the classification of single-user behavior. However, in a living or working environment, there are usually multiple inhabitants acting together, hence it makes sense to interpret the activities by considering the aggregated information from different subjects. In this paper, we address the problem of group activity recognition (GAR) in a hierarchical way by first examining individual person’s actions, reconstructed by correlating data coming from body-worn and external positioning sensors. We then aggregate this information by considering each individual as an input of a hierarchical deep belief network (DBN). This aims to extract common temporal/spatial dynamics at the level of group activity. We evaluated the proposed approach in a laboratory environment, where the participants labeled their daily activities using an app on a mobile phone. Collected data contributed to the creation of two datasets respectively containing labeled single and group activities. The experimental results evaluated on these datasets and on a public one demonstrated the effectiveness of the proposed model with respect to a support vector machine (SVM) baseline.</td></tr><tr><td>M. Ryoo</td><td>2009</td><td>Stochastic Representation and Recognition of High-Level Group Activities</td><td>118</td></tr><tr><td colspan=\"4\">This paper describes a stochastic methodology for the recognition of various types of high-level group activities. Our system maintains a probabilistic representation of a group activity, describing how individual activities of its group members must be organized temporally, spatially, and logically. In order to recognize each of the represented group activities, our system searches for a set of group members that has the maximum posterior probability of satisfying its representation. A hierarchical recognition algorithm utilizing a Markov chain Monte Carlo (MCMC)-based probability distribution sampling has been designed, detecting group activities and finding the acting groups simultaneously. The system has been tested to recognize complex activities such as ‘a group of thieves stealing an object from another group’ and ‘a group assaulting a person’. Videos downloaded from YouTube as well as videos that we have taken are tested. Experimental results show that our system recognizes a wide range of group activities more reliably and accurately, as compared to previous approaches.</td></tr><tr><td>A. Shirkhodaie</td><td>2017</td><td>Joint object and action recognition via fusion of partially observable surveillance imagery data</td><td>1</td></tr><tr><td colspan=\"4\">Partially observable group activities (POGA) occurring in confined spaces are epitomized by their limited observability of the objects and actions involved. In many POGA scenarios, different objects are being used by human operators for the conduct of various operations. In this paper, we describe the ontology of such as POGA in the context of In-Vehicle Group Activity (IVGA) recognition. Initially, we describe the virtue of ontology modeling in the context of IVGA and show how such an ontology and a priori knowledge about the classes of in-vehicle activities can be fused for inference of human actions that consequentially leads to understanding of human activity inside the confined space of a vehicle. In this paper, we treat the problem of “action-object” as a duality problem. We postulate a correlation between observed human actions and the object that is being utilized within those actions, and conversely, if an object being handled is recognized, we may be able to expect a number of actions that are likely to be performed on that object. In this study, we use partially observable human postural sequences to recognition actions. Inspired by convolutional neural networks (CNNs) learning capability, we present an architecture design using a new CNN model to learn “action-object” perception from surveillance videos. In this study, we apply a sequential Deep Hidden Markov Model (DHMM) as a post-processor to CNN to decode realized observations into recognized actions and activities. To generate the needed imagery data set for the training and testing of these new methods, we use the IRIS virtual simulation software to generate high-fidelity and dynamic animated scenarios that depict in-vehicle group activities under different operational contexts. The results of our comparative investigation are discussed and presented in detail.</td></tr><tr><td>A. Shirkhodaie</td><td>2016</td><td>Skin subspace color modeling for daytime and nighttime group activity recognition in confined operational spaces</td><td>4</td></tr><tr><td colspan=\"4\">In many military and homeland security persistent surveillance applications, accurate detection of different skin colors in varying observability and illumination conditions is a valuable capability for video analytics. One of those applications is In-Vehicle Group Activity (IVGA) recognition, in which significant changes in observability and illumination may occur during the course of a specific human group activity of interest. Most of the existing skin color detection algorithms, however, are unable to perform satisfactorily in confined operational spaces with partial observability and occultation, as well as under diverse and changing levels of illumination intensity, reflection, and diffraction. In this paper, we investigate the salient features of ten popular color spaces for skin subspace color modeling. More specifically, we examine the advantages and disadvantages of each of these color spaces, as well as the stability and suitability of their features in differentiating skin colors under various illumination conditions. The salient features of different color subspaces are methodically discussed and graphically presented. Furthermore, we present robust and adaptive algorithms for skin color detection based on this analysis. Through examples, we demonstrate the efficiency and effectiveness of these new color skin detection algorithms and discuss their applicability for skin detection in IVGA recognition applications.</td></tr><tr><td>A. Shirkhodaie</td><td>2016</td><td>In-vehicle group activity modeling and simulation in sensor-based virtual environment</td><td>5</td></tr><tr><td colspan=\"4\">Human group activity recognition is a very complex and challenging task, especially for Partially Observable Group Activities (POGA) that occur in confined spaces with limited visual observability and often under severe occultation. In this paper, we present IRIS Virtual Environment Simulation Model (VESM) for the modeling and simulation of dynamic POGA. More specifically, we address sensor-based modeling and simulation of a specific category of POGA, called In-Vehicle Group Activities (IVGA). In VESM, human-alike animated characters, called humanoids, are employed to simulate complex in-vehicle group activities within the confined space of a modeled vehicle. Each articulated humanoid is kinematically modeled with comparable physical attributes and appearances that are linkable to its human counterpart. Each humanoid exhibits harmonious full-body motion - simulating human-like gestures and postures, facial impressions, and hands motions for coordinated dexterity. VESM facilitates the creation of interactive scenarios consisting of multiple humanoids with different personalities and intentions, which are capable of performing complicated human activities within the confined space inside a typical vehicle. In this paper, we demonstrate the efficiency and effectiveness of VESM in terms of its capabilities to seamlessly generate time-synchronized, multi-source, and correlated imagery datasets of IVGA, which are useful for the training and testing of multi-source full-motion video processing and annotation. Furthermore, we demonstrate full-motion video processing of such simulated scenarios under different operational contextual constraints.</td></tr><tr><td>Tianmin Shu</td><td>2017</td><td>CERN: Confidence-Energy Recurrent Network for Group Activity Recognition</td><td>71</td></tr><tr><td colspan=\"4\">This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network &#x2013; CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.</td></tr><tr><td>Kyle Stephens</td><td>2017</td><td>Recognizing Interactions Between People from Video Sequences</td><td>0</td></tr><tr><td colspan=\"4\">This research study proposes a new approach to group activity recognition which is fully automatic. The approach adopted is hierarchical, starting with tracking and modelling local movement leading to the segmentation of moving regions. Interactions between moving regions are modelled using Kullback-Leibler (KL) divergence. Then the statistics of such movement interactions or as relative positions of moving regions is represented using kernel density estimation (KDE). The dynamics of such movement interactions and relative locations is modelled as well in a development of the approach. Eventually, the KDE representations are subsampled and considered as inputs of a support vector machines (SVM) classifier. The proposed approach does not require any intervention by an operator.</td></tr><tr><td>Yansong Tang</td><td>2019</td><td>Learning Semantics-Preserving Attention and Contextual Interaction for Group Activity Recognition</td><td>10</td></tr><tr><td colspan=\"4\">In this paper, we investigate the problem of group activity recognition by learning semantics-preserving attention and contextual interaction among different people. Conventional methods usually aggregate the features extracted from individual persons by pooling operations, which lack physical meaning and cannot fully explore the contextual information for group activity recognition. To address this, we develop a Semantics-Preserving Teacher-Student (SPTS) networks architecture. Our SPTS networks first learn a Teacher Network in the semantic domain that classifies the word of group activity based on the words of individual actions. Then, we design a Student Network in the appearance domain that recognizes the group activity according to the input video. We enforce the Student Network to mimic the Teacher Network in the learning procedure. In this way, we allocate semantics-preserving attention to different people, which is more effective to seek the key people and discard the misleading people, while no extra labeled data are required. Moreover, a group of people inherently lie in a graph-based structure, where the people and their relationship can be regarded as the nodes and edges of a graph, respectively. Based on this, we build two graph convolutional modules on both the Teacher Network and the Student Network to reason the dependency among different people. Furthermore, we extend our approach on action segmentation task based on its intermediate features. The experimental results on four datasets for group activity analysis clearly show the superior performance of our method in comparison with the state-of-the-art.</td></tr><tr><td>Yansong Tang</td><td>2018</td><td>Mining Semantics-Preserving Attention for Group Activity Recognition</td><td>14</td></tr><tr><td colspan=\"4\">In this paper, we propose a Semantics-Preserving Teacher-Student (SPTS) model for group activity recognition in videos, which aims to mine the semantics-preserving attention to automatically seek the key people and discard the misleading people. Conventional methods usually aggregate the features extracted from individual persons by pooling operations, which cannot fully explore the contextual information for group activity recognition. To address this, our SPTS networks first learn a Teacher Network in semantic domain, which classifies the word of group activity based on the words of individual actions. Then we carefully design a Student Network in vision domain, which recognizes the group activity according to the input videos, and enforce the Student Network to mimic the Teacher Network during the learning process. In this way, we allocate semantics-preserving attention to different people, which adequately explores the contextual information of different people and requires no extra labelled data. Experimental results on two widely used benchmarks for group activity recognition clearly show the superior performance of our method in comparisons with the state-of-the-arts.</td></tr><tr><td>Yansong Tang</td><td>2020</td><td>Graph Interaction Networks for Relation Transfer in Human Activity Videos</td><td>2</td></tr><tr><td colspan=\"4\">Recent years have witnessed rapid progress in employing graph convolutional networks (GCNs) for various video analysis tasks where graph-based data abound. However, exploring the transferable knowledge between different graphs, which is a direction with wide and potential applications, has been rarely studied. To address this issue, we propose a graph interaction networks (GINs) model for transferring relation knowledge across two graphs. Different from conventional domain adaptation or knowledge distillation approaches, our GINs focus on a “self-learned” weight matrix, which is a higher-level representation of the input data. And each element of the weight matrix represents the pair-wise relation among different nodes within the graph. Moreover, we guide the networks to transfer the knowledge across the weight matrices by designing a task-specific loss function, so that the relation information is well preserved during transfer. We conduct experiments on two different scenarios for video analysis, including a new proposed setting for unsupervised skeleton-based action recognition across different datasets, and supervised group activity recognition with multi-modal inputs. Extensive experiments on six widely used datasets illustrate that our GINs achieve very competitive performance in comparison with the state-of-the-arts.</td></tr><tr><td>Moumita Roy Tora</td><td>2017</td><td>Classification of Puck Possession Events in Ice Hockey</td><td>26</td></tr><tr><td colspan=\"4\">Group activity recognition in sports is often challenging due to the complex dynamics and interaction among the players. In this paper, we propose a recurrent neural network to classify puck possession events in ice hockey. Our method extracts features from the whole frame and appearances of the players using a pre-trained convolutional neural network. In this way, our model captures the context information, individual attributes and interaction among the players. Our model requires only the player positions on the image and does not need any explicit annotations for the individual actions or player trajectories, greatly simplifying the input of the system. We evaluate our model on a new Ice Hockey Dataset. Experimental results show that our model produces competitive results on this challenging dataset with much simpler inputs compared with the previous work.</td></tr><tr><td>Khai N. Tran</td><td>2014</td><td>Activity analysis in crowded environments using social cues for group discovery and human interaction modeling</td><td>45</td></tr><tr><td colspan=\"4\">We propose a graph based algorithm with social cues to discover interacting groups.We propose a novel descriptor to recognize group activity in top-down approach.The descriptor capturing the motion and interaction of people in discovered group.The advantage of social cue in better understanding of activities in crowded scene. This paper presents a novel and efficient framework for group activity analysis. People in a scene can be intuitively represented by an undirected graph where vertices are people and the edges between two people are weighted by how much they are interacting. Social signaling cues are used to describe the degree of interaction between people. We propose a graph-based clustering algorithm to discover interacting groups in crowded scenes. Two social signaling cues are presented and compared for group discovery. The grouping of people in the scene serves to isolate the groups engaged in the dominant activity, effectively eliminating dataset contamination. Using discovered interacting groups, we create a descriptor capturing the motion and interaction of people within it. A bag-of-words approach is used to represent group activity and a SVM classifier is used for activity recognition. The proposed framework is evaluated in its ability to discover interacting groups and perform group activity recognition using two public datasets. The overall recognition system is compared to a baseline top-down model to understand the impact of social cues for activity recognition. The results of both the steps show that our method outperforms state-of-the-art methods for group discovery and achieves recognition rates comparable to state-of-the-art methods for group activity recognition.</td></tr><tr><td>Minsi Wang</td><td>2017</td><td>Recurrent Modeling of Interaction Context for Collective Activity Recognition</td><td>52</td></tr><tr><td colspan=\"4\">Modeling of high order interactional context, e.g., group interaction, lies in the central of collective/group activity recognition. However, most of the previous activity recognition methods do not offer a flexible and scalable scheme to handle the high order context modeling problem. To explicitly address this fundamental bottleneck, we propose a recurrent interactional context modeling scheme based on LSTM network. By utilizing the information propagation/aggregation capability of LSTM, the proposed scheme unifies the interactional feature modeling process for single person dynamics, intra-group (e.g., persons within a group) and inter-group(e.g., group to group)interactions. The proposed high order context modeling scheme produces more discriminative/descriptive interactional features. It is very flexible to handle a varying number of input instances (e.g., different number of persons in a group or different number of groups) and linearly scalable to high order context modeling problem. Extensive experiments on two benchmark collective/group activity datasets demonstrate the effectiveness of the proposed method.</td></tr><tr><td>Chonthisa Wateosot</td><td>2019</td><td>Group activity recognition with an interaction force based on low‐level features</td><td>2</td></tr><tr><td colspan=\"4\">None</td></tr><tr><td>Jianchao Wu</td><td>2019</td><td>Learning Actor Relation Graphs for Group Activity Recognition</td><td>56</td></tr><tr><td colspan=\"4\">Modeling relation between actors is important for recognizing group activity in a multi-person scene. This paper aims at learning discriminative relation between actors efficiently using deep models. To this end, we propose to build a flexible and efficient {\\rm Actor Relation Graph} (ARG) to simultaneously capture the appearance and position relation between actors. Thanks to the Graph Convolutional Network, the connections in ARG could be automatically learned from group activity videos in an end-to-end manner, and the inference on ARG could be efficiently performed with standard matrix operations. Furthermore, in practice, we come up with two variants to sparsify ARG for more effective modeling in videos: spatially localized ARG and temporal randomized ARG. We perform extensive experiments on two standard group activity recognition datasets: the Volleyball dataset and the Collective Activity dataset, where state-of-the-art performance is achieved on both datasets. We also visualize the learned actor graphs and relation features, which demonstrate that the proposed ARG is able to capture the discriminative relation information for group activity recognition.</td></tr><tr><td>Lifang Wu</td><td>2020</td><td>Fusing Motion Patterns and Key Visual Information for Semantic Event Recognition in Basketball Videos</td><td>3</td></tr><tr><td colspan=\"4\">Abstract Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance.</td></tr><tr><td>Zhao Xie</td><td>2019</td><td>Jointly social grouping and identification in visual dynamics with causality-induced hierarchical Bayesian model</td><td>10</td></tr><tr><td colspan=\"4\">Abstract We concentrate on modeling the person-person interactions for group activity recognition. In order to solve the complexity and ambiguity problems caused by a large number of human objects, we propose a causality-induced hierarchical Bayesian model to tackle the interaction activity video, referring to the “what” interaction activities happen, “where” interaction atomic occurs in spatial, and “when” group interaction happens in temporal. In particular, Granger Causality has been characterized with multiple features to encode the interacting relationships between each individual in the group. Furthermore, to detect and identify the concurrent interactive simultaneously, we investigate the Relative Entropy as a metric to measure the reasonable motion dependency between two arbitrary individuals. Filtered by the causality dependency, causality motion features have been cast as the multiplicative probabilistic ingredients in Bayesian factors to formulate the compact learned latent interaction patterns aggregately that enable the power of discrimination. Experiments demonstrate our model outperforms state-of-the-art models.</td></tr><tr><td>Dezhong Xu</td><td>2020</td><td>Group Activity Recognition by Using Effective Multiple Modality Relation Representation With Temporal-Spatial Attention</td><td>1</td></tr><tr><td colspan=\"4\">Group activity recognition has received a great deal of interest because of its broader applications in sports analysis, autonomous vehicles, CCTV surveillance systems and video summarization systems. Most existing methods typically use appearance features and they seldom consider underlying interaction information. In this work, a technology of novel group activity recognition is proposed based on multi-modal relation representation with temporal-spatial attention. First, we introduce an object relation module, which processes all objects in a scene simultaneously through an interaction between their appearance feature and geometry, thus allowing the modeling of their relations. Second, to extract effective motion features, an optical flow network is fine-tuned by using the action loss as the supervised signal. Then, we propose two types of inference models, opt-GRU and relation-GRU, which are used to encode the object relationship and motion representation effectively, and form the discriminative frame-level feature representation. Finally, an attention-based temporal aggregation layer is proposed to integrate frame-level features with different weights and form effective video-level representations. We have performed extensive experiments on two popular datasets, and both have achieved state-of-the-art performance. The datasets are the Volleyball dataset and the Collective Activity dataset, respectively.</td></tr><tr><td>Rui Yan</td><td>2018</td><td>Participation-Contributed Temporal Dynamic Model for Group Activity Recognition</td><td>21</td></tr><tr><td colspan=\"4\">Group activity recognition, a challenging task that a number of individuals occur in the scene of activity while only a small subset of them participate in, has received increasing attentions. However, most of the previous methods model all the individuals' actions equivalently while ignoring a fact that not all of them are contributed to the discrimination of group activity. That is to say, only a small number of key actors (participants) play important roles in the whole group activity. Inspired by this, we explore a new \"One to Key\" idea to progressively aggregate temporal dynamics of key actors with different participation degrees over time from each person. Here, we focus on two types of key actors in the whole activity, who steadily move in the whole process (long moving time) or intensely move (but closely related to the group activity) at a significant moment. Based on this, we propose a novel Participation-Contributed Temporal Dynamic Model (PC-TDM) to recognize group activity, which mainly consists of a \"One\" network and a \"One to Key\" network. Specifically, \"One\" network aims at modeling the individual dynamic of each person. \"One to Key\" network feeds the outputs from the \"One\" network into a Bidirectional LSTM (Bi-LSTM) according to the order of individual's moving time. Subsequently, each output state of Bi-LSTM weighted by a trainable time-varying attention factor is aggregated by going through LSTM one-by-one. Experimental results on two benchmarks demonstrate that the proposed method improves group activity recognition performance compared to the state-of-the-arts.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML('<table><tr><th>Author</th><th>Year</th><th>Title</th><th>Citation Count</th><tr>{}</tr></table>'.format(\n",
    "                '</tr><tr>'.join(\n",
    "                    '<td>{}{}{}{}{}{}{}</td></tr><tr><td colspan=\"4\">{}</td>'.format(paper['authors'][0]['name'], '</td><td>', paper['year'] ,'</td><td>', paper['title'] , \n",
    "                                                 '</td><td>', paper['numCitedBy'] , paper['abstract'])\n",
    "                for paper in papers)\n",
    "            )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lessons Learnt\n",
    "\n",
    "- Ways to open an file and read it line by line, in contrast to reading in every line into a list or string\n",
    "- Beginning in using regular expressions is relatively straigthtforward\n",
    "- Displaying HTML and table is potentially useful\n",
    "- Debugging in Jupyter is painful - need to look at other ways to write python initally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.1109/aina.2016.94', '10.1186/s13174-019-0103-1', '10.1145/3295747', '10.1007/978-3-030-30645-8_26', '10.1145/3316615.3316722', '10.1109/cvpr.2019.00808', '10.1007/s00521-016-2346-0', '10.1109/mdm.2014.62', '10.1109/compsac.2016.202', '10.1016/j.autcon.2019.102886', '10.1145/3311957.3359471', '10.1142/s0218001415550071', '10.1109/cvpr.2016.516', '10.1117/12.919946', '10.1117/12.2018626', '10.1117/12.2050909', '10.1016/j.procs.2019.11.212', '10.3390/fi12080133', '10.1007/978-3-030-20887-5_21', '10.1109/iccv.2003.1238423', '10.1007/s11036-012-0415-x', '10.1109/tpami.2016.2613865', '10.1109/iccv.2015.462', '10.1007/978-3-030-01219-9_44', '10.1109/cvpr.2016.217', '10.1007/978-3-319-24078-7_16', '10.1016/j.patcog.2017.10.037', '10.1109/icpr.2014.605', '10.1109/tpami.2011.228', '10.1109/wacv.2017.31', '10.1007/s11263-012-0573-0', '10.1109/iccv.2017.313', '10.1109/tcsvt.2013.2280849', '10.1109/tcsvt.2013.2269780', '10.1109/access.2019.2929684', '10.1109/iccvw.2013.71', '10.1117/12.2178547', '10.1007/978-3-030-01249-6_7', '10.1109/tcsvt.2019.2894161', '10.1007/s12652-020-01851-0', '10.1007/s11263-010-0355-5', '10.1117/12.2266224', '10.1117/12.2226026', '10.1117/12.2226029', '10.1109/cvpr.2017.453', '10.1007/978-3-319-64689-3_7', '10.1109/tip.2019.2914577', '10.1145/3240508.3240576', '10.1109/tcsvt.2020.2973301', '10.1109/cvprw.2017.24', '10.1016/j.patrec.2013.09.015', '10.1109/cvpr.2017.783', '10.1002/tee.22901', '10.1109/cvpr.2019.01020', '10.1016/j.neucom.2020.07.003', '10.1016/j.jvcir.2019.01.006', '10.1109/access.2020.2979742', '10.1145/3240508.3240572']\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0,len(papers)-1):\n",
    "#        print(f\"{i} - {papers[i]['title']}\")\n",
    "\n",
    "#print(papers[54])\n",
    "print(DOI_List)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
