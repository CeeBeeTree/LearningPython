{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTH Actions Data Set (Video files) - Loading and Classification\n",
    "**Author:** Christian Byron  **Date:** 30-Jun-21\n",
    "\n",
    "This notebook provides an example of loading and classifying the KTH Action dataset in its original form (avi video). This is the second clssification activity for this dataset and as such will focus on a reduced dataset with only three of the original  six actions (walk, jog, run). These actions have low accuracy in the prior classification experiement due to similarity of viewing them as static images.\n",
    "\n",
    "### Step 1 - Define a dataset class to view and load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "import random \n",
    "import cv2\n",
    "\n",
    "class KTHA2_dataset:\n",
    "\n",
    "        def __init__(self):\n",
    "            self.__base_video_path =  \"C:\\\\Users\\\\s441606\\\\Documents\\\\Videos\\\\KTHA\"\n",
    "            self.__data = []\n",
    "            \n",
    "            label_dirs = [ d for d in listdir(self.__base_video_path) if \n",
    "                          not(path.isfile(path.join(self.__base_video_path, d)))]\n",
    "            \n",
    "            for d in label_dirs:\n",
    "                for f in listdir(path.join(self.__base_video_path, d)):\n",
    "                    self.__data.append( (f, d) )\n",
    "                    \n",
    "            self.__small_sample_data = random.sample(self.__data, 10)\n",
    "            \n",
    "        def video_list(self): \n",
    "            # Get a random sample of 10\n",
    "            return [i[0] for i in self.__small_sample_data]\n",
    "\n",
    "        def video_annotations(self, video_ref):\n",
    "            return [i[1] for i in self.__small_sample_data if i[0] == video_ref]\n",
    "        \n",
    "        def video_annotation_frame_num(self,video_ref, annotation_ref):\n",
    "            return 0\n",
    "        \n",
    "        def video_end_frame_num(self, video_ref):\n",
    "            dir = self.video_annotations(video_ref)[0]\n",
    "            path = self.__base_video_path + \"\\\\\" + dir +  \"\\\\\" +  video_ref \n",
    "            cap = cv2.VideoCapture(path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            return total_frames\n",
    "        \n",
    "        def video_start_frame_num(self, video_ref):\n",
    "            return 0\n",
    "        \n",
    "        def video_frame(self, video_ref, frame_num):\n",
    "            dir = self.video_annotations(video_ref)[0]\n",
    "            path = self.__base_video_path + \"\\\\\" + dir +  \"\\\\\" +  video_ref \n",
    "           \n",
    "            cap = cv2.VideoCapture(path)\n",
    "            cap.set(1, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            is_success, im_buf_arr = cv2.imencode(\".jpg\", frame)\n",
    "            byte_im = im_buf_arr.tobytes()\n",
    "            cap.release()\n",
    "            return byte_im\n",
    "                                    \n",
    "        def video_annotated_frame(self, video_ref, annotation_ref, frame_num):\n",
    "            return self.video_frame(video_ref, frame_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Display the Video Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26558f78d2ed4925a0d13f753d3d777e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Video #:', options=('person08_walking_d4_uncomp.avi', 'perâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from VideoDatasetUI import VideoDatasetUI\n",
    "\n",
    "ds = KTHA2_dataset()\n",
    "videoUI = VideoDatasetUI(ds)\n",
    "videoUI.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
