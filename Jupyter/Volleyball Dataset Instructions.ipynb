{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Reconstruction for Volleyball dataset\n",
    "**Author:** Christian Byron  **Date:** 26-Mar-21\n",
    "\n",
    "This notebook provides an understanding of the Volleyball dataset by reconstructing it into a video gui. The volleyball dataset (available [here](https://github.com/mostafa-saad/deep-activity-rec#dataset)) was created by Ibrahim et al. <sub>[1]</sub> using publicly available YouTube volleyball videos.\n",
    "\n",
    "- [x] Figure out how to change GUI to select the video from the list when established form the subdirectories\n",
    "- [x] Change the frames collection to be driven by selecting video ... for now we just provide all of the video frames\n",
    "- [ ] Add next steps to read in the annotations \n",
    "- [x] Some excepion handling on widgets when moving back to prior videos (upsets min and max values).. May mean redesigning the video data store again. \n",
    "\n",
    "### Step 1 - Find all videos and individual frames\n",
    "\n",
    "- The dataset contains 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. \n",
    "- Each video has a directory for it with sequential IDs (0, 1...54)\n",
    "- Inside each video directory, a set of directories corresponds to annotated frames (e.g. volleyball/39/29885)\n",
    "- Each frame directory has 41 images (20 images before target frame, target frame, 20 frames after target frame)\n",
    "- Each video directory has annotations.txt file that contains selected frames annotations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "base_video_collection_path = \\\n",
    "    \"C:\\\\Users\\\\s441606\\\\OneDrive - University of Canberra\\\\2021 Semester 1\\\\z- Learning Python\\\\Jupyter\\\\videos\"\n",
    "\n",
    "video_folder_list = os.listdir(base_video_collection_path)\n",
    "videos = {}\n",
    "\n",
    "for video_folder in video_folder_list:\n",
    "\n",
    "    video_starting_frame = sys.maxsize\n",
    "    video_ending_frame = 0\n",
    "    video_frames = {}\n",
    "    video_annotations_all = {}\n",
    "    video_annotations_options = list()\n",
    "    \n",
    "    video_annotations_file = open(base_video_collection_path + \"\\\\\" + video_folder + '\\\\annotations.txt', 'r')\n",
    "    \n",
    "    for video_annotations_line in video_annotations_file:\n",
    "        video_annotation_line_search = re.search('\\d+', video_annotations_line)\n",
    "        if video_annotation_line_search: video_annotations_all.update({video_annotation_line_search.group(0): \n",
    "                                                                       video_annotations_line})\n",
    "    \n",
    "    \n",
    "    for subdir, dirs, files in os.walk(base_video_collection_path + \"\\\\\" + video_folder):\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                video_frame_number = os.path.splitext(file)[0]\n",
    "                video_starting_frame = min(int(video_frame_number), video_starting_frame)\n",
    "                video_ending_frame = max(int(video_frame_number), video_ending_frame)\n",
    "                video_frames.update({int(video_frame_number):os.path.join(subdir, file)})\n",
    "                \n",
    "                if video_frame_number in video_annotations_all :\n",
    "                    video_annotation_line_split = re.split('\\s', video_annotations_all[video_frame_number])\n",
    "                    video_annotation_line_split[0] = video_frame_number\n",
    "                    video_annotations_options.append((video_frame_number + ' ' + video_annotation_line_split[1], \n",
    "                                                      video_annotation_line_split))\n",
    "               \n",
    "    video_obj = {\"starting_frame\":video_starting_frame, \"ending_frame\":video_ending_frame,\"frames\":video_frames,\n",
    "                 \"annotations_options\":video_annotations_options} \n",
    "    \n",
    "    videos.update({video_folder:video_obj})\n",
    "\n",
    "#display(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Build the GUI Function to show video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126468937b4941759a6359fa18661158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Video #:', options=('0', '1'), value='0'), Play(value=3576…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "imageArea = widgets.Image()\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options= video_folder_list,\n",
    "    value ='1',\n",
    "    description='Video #:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "play = widgets.Play(\n",
    "        step=1,\n",
    "        interval=50,\n",
    "        description=\"Press play\",\n",
    "        disabled=False\n",
    ")\n",
    "\n",
    "slider = widgets.IntSlider()\n",
    "\n",
    "widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "\n",
    "def on_dropdown_value_change(change): \n",
    "    video_selected = videos[change['new']]\n",
    "    video_start = video_selected[\"starting_frame\"]\n",
    "    video_end = video_selected[\"ending_frame\"]\n",
    "    \n",
    "    slider.min = play.min = 0\n",
    "    slider.max = play.max = video_end\n",
    "    slider.min = play.min = video_start\n",
    "    slider.value = video_start\n",
    "    \n",
    "    annotations.options = video_selected['annotations_options']\n",
    "  \n",
    "  \n",
    "dropdown.observe(on_dropdown_value_change, names='value')\n",
    "\n",
    "def on_value_change(change):\n",
    "    video_selected = videos[dropdown.value]\n",
    "    \n",
    "    file = open(f\"{video_selected['frames'][change['new']]}\",\"rb\")\n",
    "    imageArea.value = file.read()\n",
    " \n",
    "slider.observe(on_value_change, names='value')\n",
    "\n",
    "annotations = widgets.Dropdown(\n",
    "    options= videos[dropdown.value]['annotations_options'],\n",
    "    description='Annotated Frame #:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "controls = widgets.VBox([widgets.HBox([dropdown, play, slider,annotations]),imageArea])\n",
    "\n",
    "# trigger the initial video to correctly set the slider values and load the first image\n",
    "dropdown.value = '0'\n",
    "controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Update the display when an Annotation is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def on_annotations_dropdown_value_change(change): \n",
    "    annotations_list = change['new']\n",
    "    frame_number = int(annotations_list[0])\n",
    "    play._playing = False\n",
    "    slider.value = frame_number\n",
    "    \n",
    "    # convert the current frame image to a numpy array for opencv\n",
    "    nparr = np.frombuffer(imageArea.value, np.uint8)\n",
    "    im = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    #loop through each annotation set in the list 9 set of five values each\n",
    "    for i in range(2,len( annotations_list) -5 ,5):\n",
    "        start_point = (int(annotations_list[i]), int(annotations_list[i+1]))\n",
    "\n",
    "        end_point = (int(annotations_list[i]) + int(annotations_list[i+2]), \n",
    "                     int(annotations_list[i+1]) + int(annotations_list[i+3]))\n",
    "        \n",
    "        im = cv2.rectangle(im, start_point, end_point, (255, 0, 0), 2)\n",
    "        im = cv2.putText(im, annotations_list[i+4], start_point, cv2.FONT_HERSHEY_SIMPLEX , \n",
    "                         1, (255, 0, 0), 1)\n",
    "        \n",
    "   \n",
    "    is_success, im_buf_arr = cv2.imencode(\".jpg\", im)\n",
    "    byte_im = im_buf_arr.tobytes()\n",
    "\n",
    "    imageArea.value = byte_im\n",
    "    \n",
    "    \n",
    "\n",
    "annotations.observe(on_annotations_dropdown_value_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lessons Learnt\n",
    "- A lot of learning about the iPython widgets (inc lack of documentation on class interfaces). Especially on how to use observe method to select and run the video(cascading value changes)\n",
    "- How to show a image using ipyWidgets - may be useful later - or may be replaced with other image libraries (eg Matlablib)\n",
    "- A bit of a challenge to create the complex store of videos in a dictionary\n",
    "- Folders are named after the annotated frame - not the beginning or end. So need to process differently to get the correct details\n",
    "- Refresh syntax for regular expressions - used https://regex101.com/ as means to test and the best cheatsheet https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions/Cheatsheet\n",
    "- Started with openCV and use in jupyter - discovered natively cv2.imshow opens new windows :-P ; where as to open in Image widget needs conversion from bytes to numpy array (useful sites https://jdhao.github.io/2019/07/06/python_opencv_pil_image_to_bytes/ and https://intellipaat.com/community/15647/python-opencv-load-image-from-byte-string\n",
    "\n",
    "\n",
    "#### References\n",
    "[1]  Mostafa S. Ibrahim et al. “A Hierarchical Deep Temporal Model for Group Activity Recognition”. In:2016 IeeeConference on Computer Vision and Pattern Recognition.  IEEE  Conference  on  Computer  Vision  and  PatternRecognition.  2016,  pp.  1971–1980.isbn:  978-1-4673-8851-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
